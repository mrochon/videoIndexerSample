{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Queue Wait Time\n",
    "\n",
    "### Goal\n",
    "\n",
    "Uses image analysis of in-store video to estimate current wait time for new arrivals.\n",
    "\n",
    "### Set initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint='https://mrai.openai.azure.com/'\n",
    "azure_openai_api_version='2024-05-01-preview'\n",
    "azure_openai_chat_deployment='gpt-4o-mini'\n",
    "azure_openai_key = os.environ.get(\"AZURE_OPENAI_KEY\") \n",
    "\n",
    "framesPerHour = 60\n",
    "compressionPercent = 50\n",
    "video_path = '../Data/waiting in line.mp4'\n",
    "avgServiceTimeInSec = 30\n",
    "serviceStations = 1\n",
    "\n",
    "sysPrompt = f\"\"\"\n",
    "Your job is to determine the number of people in the provided image. Respond with just a single number indicating the number of people in the image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire video frame\n",
    "\n",
    "Extract single frames from the video every *3600/framesPerHour* seconds and save them as jpg files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1 frames from the video.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder, interval):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Get the frames per second (fps) of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(fps * interval)\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Set the position of the next frame to capture\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count * frame_interval)\n",
    "        # Read the next frame\n",
    "        success, frame = cap.read()\n",
    "        # If the frame was not successfully read, we've reached the end of the video\n",
    "        if not success:\n",
    "            break\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{frame_count}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame, [cv2.IMWRITE_JPEG_QUALITY, compressionPercent])\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames from the video.\")\n",
    "\n",
    "output_folder = '../Data/frames'\n",
    "extract_frames(video_path, output_folder, 3600/framesPerHour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AI client\n",
    "\n",
    "**NOT USED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "\n",
    "# from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# openai_credential = DefaultAzureCredential()\n",
    "# token_provider = get_bearer_token_provider(openai_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    #azure_ad_token_provider=azure_openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get people count\n",
    "\n",
    "Call gpt4o-mini to get count of people in a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People in line: 6\n",
      "Input  tokens: 36878\n",
      "Output tokens: 1\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import requests  \n",
    "import base64  \n",
    "from azure.identity import ClientSecretCredential  \n",
    "  \n",
    "# Configuration  \n",
    "TENANT_ID = \"YOUR_TENANT_ID\"  \n",
    "CLIENT_ID = \"YOUR_CLIENT_ID\"  \n",
    "CLIENT_SECRET = \"YOUR_CLIENT_SECRET\"  \n",
    "RESOURCE = \"https://management.azure.com/.default\"  \n",
    "IMAGE_PATH = \"../Data/frames/frame_0.jpg\"\n",
    "  \n",
    "# Authenticate and get token  \n",
    "# credential = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)  \n",
    "# token = credential.get_token(RESOURCE).token  \n",
    "  \n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')  \n",
    "  \n",
    "headers = {  \n",
    "    \"Content-Type\": \"application/json\",  \n",
    "    #\"Authorization\": f\"Bearer {token}\"  \n",
    "    \"api-key\": azure_openai_key\n",
    "}  \n",
    "  \n",
    "# Payload for the request  \n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": sysPrompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"\\n\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"\\n\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800,\n",
    "  #\"response_format\": \"ResponseFormatJsonObject\"\n",
    "}  \n",
    "ENDPOINT = \"https://mrai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview\"  \n",
    "  \n",
    "# Send request  \n",
    "try:  \n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)  \n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code  \n",
    "except requests.RequestException as e:  \n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")  \n",
    "  \n",
    "print(f\"People in line: {response.json()['choices'][0]['message']['content']}\")\n",
    "print(f\"Input  tokens: {response.json()['usage']['prompt_tokens']}\")\n",
    "print(f\"Output tokens: {response.json()['usage']['completion_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI class\n",
    "\n",
    "**NOT USED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='0', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../Data/frames/frame_0.jpg\"\n",
    "encoded_image = base64.b64encode(open(image_path, 'rb').read()).decode('ascii')  \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=azure_openai_chat_deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sysPrompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\\n\"\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                }\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\\n\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "Based on [pricing per this doc](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/openai%E2%80%99s-gpt-4o-mini-now-available-in-api-with-vision-and-fine-tuning-text-capab/4200640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost per store per hour is: $0.36\n"
     ]
    }
   ],
   "source": [
    "costPer1KInput = 0.00015\n",
    "costPer1KOutput = 0.0006\n",
    "inputTokensPerFrame = 40000\n",
    "outputTokensPerFrame = 10\n",
    "\n",
    "costPerHour = framesPerHour*(costPer1KInput*inputTokensPerFrame*compressionPercent/100 + costPer1KOutput*outputTokensPerFrame)/1000\n",
    "# Compression makes the image smaller but does not change number of input tokens. Why?\n",
    "costPerHour = framesPerHour*(costPer1KInput*inputTokensPerFrame + costPer1KOutput*outputTokensPerFrame)/1000\n",
    "print(f\"The cost per store per hour is: ${costPerHour:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
